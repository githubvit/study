爬虫的定义：

    向网站发起请求，获取资源后分析并提取有用数据的程序 

爬取的过程：

    发送请求 ———> 获取响应 ———> 解析内容 ———> 保存数据

    #1、发起请求
        使用http库向目标站点发起请求，即发送一个Request
        Request包含：请求头、请求体等

    #2、获取响应内容
        如果服务器能正常响应，则会得到一个Response
        Response包含：响应头、响应体（html，json，图片，视频）等

    #3、解析内容
        解析html数据：正则表达式，第三方解析库如Beautifulsoup，pyquery等
        解析json数据：json模块
        解析二进制数据:以b的方式写入文件

    #4、保存数据
        数据库
        文件

因此，学习爬虫就是学习三大类库

1. 请求库

2. 解析库

3. 存储库

# 抓包分析 

    1. 填入地址
    2. 用浏览器的右击检查或F12打开 调式分析窗口 
    3. 选择Network 选择 preserve log 保存以前的日志，因为，有可能在抓包过程中，地址跳转了，以前的结果都没了。
    4. 在地址栏回车发送请求
        看到页面内容拿下，排在第一位的就是第一个请求，后面的请求都是由这个请求引发的。
        先说http请求Request需要关注的东西：
            请求的url: url及其中文编码编码
            请求方法：
            请求体：
                Post请求才有请求体，因为get请求把参数放入地址中，post请求把参数要放入请求体中。
                所以get请求参数有大小限制1kb，而post请求参数就没有限制。

                如果是post方式，请求体是format data

                ps：
                1、登录窗口，文件上传等，信息都会被附加到请求体内
                2、登录，输入错误的用户名密码，然后提交，就可以看到post，正确登录后页面通常会跳转，无法捕捉到post

            请求头：
                cookie、
                user-agent（浏览器）、
                Referer（从哪个网站跳过来的，比如拉勾网，对于不是从本网跳转过来的求职信息一律封杀，并且给的提示是当前访问太频繁，请稍后访问，你还以为没被禁止。）

        再说http响应Response要关注的：
            响应状态：
                200：代表成功
                301：代表跳转
                404：文件不存在
                403：权限
                502：服务器错误

                注意：一般网站为了迷惑爬虫，没成功也会报200，因此200意义不大。
                而301要注意，这就是为什么在抓包过程要选preserve log的原因，这种事请非常多，比如登录，登录成功就会跳转。要注意跳转的地址即location.
                看到301就要知道，这个页面已经被重定向了，重定向的url就是location。
                如果请求库没有重定向功能，就要自己用requests向location发请求，模拟重定向。
            响应头：
                cookie，
                location    重定向

            响应体：
                html(静态页面) ：用正则匹配想要的内容。
                json（ajax后台请求的动态数据）：用json模块反序列化获取想要的数据。
                二进制b（图片、视频等）：以wb方式打开一个文件，把该二进制写入该文件，即可。